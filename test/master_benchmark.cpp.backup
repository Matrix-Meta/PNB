#include "neurobit/core/types.hpp"
#include "neurobit/layers/bit_linear.hpp"
#include "neurobit/core/xmx.hpp"
#include "neurobit/layers/ssm_scan.hpp"
#include "neurobit/layers/spike_neuron.hpp"
#include "neurobit/layers/bit_brain.hpp"
#include "neurobit/components/glial.hpp"
#include "neurobit/components/hippocampus.hpp"

#include <sycl/sycl.hpp>
#include <vector>
#include <iostream>
#include <format>
#include <cmath>
#include <random>
#include <string>
#include <chrono>

namespace s = sycl;

// 測試結果結構
struct TestResult
{
    std::string name;
    bool passed;
    std::string message;
};

// 測試計數器
static int total_tests = 0;
static int passed_tests = 0;

void report_test(const TestResult &result)
{
    total_tests++;
    if (result.passed)
    {
        passed_tests++;
        std::cout << "  [PASS] {}\n" << result.name << "\n";
    }
    else
    {
        std::cout << "  [FAIL] {} - {}\n" << result.name, result.message << "\n";
    }
}

// ============================================================
// Test 1: Types and Constants
// ============================================================
void test_types()
{
    std::cout << "\n=== Test 1: Types and Constants ===\n";

    // Test PI constant
    {
        bool passed = std::abs(neurobit::PI - 3.14159265f) < 1e-5f;
        report_test({"PI constant", passed, "Value mismatch"});
    }

    // Test EPSILON constant
    {
        bool passed = neurobit::EPSILON == 1e-6f;
        report_test({"EPSILON constant", passed, "Value mismatch"});
    }

    // Test stable_coeff (tanh)
    {
        float result = neurobit::stable_coeff(0.0f);
        bool passed = std::abs(result) < 1e-6f;
        report_test({"stable_coeff(0) = 0", passed, "Expected 0"});
    }

    {
        float result = neurobit::stable_coeff(100.0f);
        bool passed = std::abs(result - 1.0f) < 1e-5f;
        report_test({"stable_coeff(large) ≈ 1", passed, "Expected ~1"});
    }

    // Test Vector1D type exists (compile-time check passed if we get here)
    {
        float data[4] = {1, 2, 3, 4};
        neurobit::Vector1D<float> vec(data, 4);
        bool passed = vec[0] == 1.0f && vec[3] == 4.0f;
        report_test({"Vector1D type", passed, "Access failed"});
    }

    // Test MatrixView type
    {
        float data[6] = {1, 2, 3, 4, 5, 6};
        neurobit::MatrixView<float> mat(data, 2, 3);
        bool passed = mat(0, 0) == 1.0f && mat(1, 2) == 6.0f;
        report_test({"MatrixView type", passed, "Access failed"});
    }

    // Test Tensor3D type
    {
        float data[8] = {1, 2, 3, 4, 5, 6, 7, 8};
        neurobit::Tensor3D<float> tensor(data, 2, 2, 2);
        bool passed = tensor(0, 0, 0) == 1.0f && tensor(1, 1, 1) == 8.0f;
        report_test({"Tensor3D type", passed, "Access failed"});
    }
}

// ============================================================
// Test 2: BitLinear Layer
// ============================================================
void test_bit_linear(s::queue &q)
{
    std::cout << "\n=== Test 2: BitLinear Layer ===\n";

    using namespace neurobit::layers;

    const size_t batch = 2;
    const size_t in_features = 4;
    const size_t out_features = 3;

    BitLinearXMX layer({batch, in_features, out_features});

    // Input: all ones
    std::vector<float> h_X(batch * in_features, 1.0f);

    // Weights: alternating +1, -1
    std::vector<int8_t> h_W(in_features * out_features);
    for (size_t i = 0; i < h_W.size(); i++)
    {
        h_W[i] = (i % 2 == 0) ? 1 : -1;
    }

    std::vector<float> h_Y(batch * out_features, 0.0f);

    {
        s::buffer<float, 1> buf_X{h_X};
        s::buffer<int8_t, 1> buf_W{h_W};
        s::buffer<float, 1> buf_Y{h_Y};

        layer.forward(q, buf_X, buf_W, buf_Y);
        q.wait();
    }

    // Verify: with alternating weights and all-ones input
    // Each output should be sum of +1 and -1 alternating
    // For in_features=4: +1 -1 +1 -1 = 0 for even cols, etc.
    bool passed = true;
    for (size_t b = 0; b < batch; b++)
    {
        for (size_t n = 0; n < out_features; n++)
        {
            // Expected: sum of weights for this output neuron
            float expected = 0.0f;
            for (size_t k = 0; k < in_features; k++)
            {
                expected += h_W[k * out_features + n];
            }
            if (std::abs(h_Y[b * out_features + n] - expected) > 1e-5f)
            {
                passed = false;
            }
        }
    }
    report_test({"BitLinear forward", passed, "Output mismatch"});

    // Test single forward
    std::vector<float> h_X_single(in_features, 2.0f);
    std::vector<float> h_Y_single(out_features, 0.0f);
    {
        s::buffer<float, 1> buf_X{h_X_single};
        s::buffer<int8_t, 1> buf_W{h_W};
        s::buffer<float, 1> buf_Y{h_Y_single};

        layer.forward_single(q, buf_X, buf_W, buf_Y);
        q.wait();
    }

    passed = true;
    for (size_t n = 0; n < out_features; n++)
    {
        float expected = 0.0f;
        for (size_t k = 0; k < in_features; k++)
        {
            expected += 2.0f * h_W[k * out_features + n];
        }
        if (std::abs(h_Y_single[n] - expected) > 1e-5f)
        {
            passed = false;
        }
    }
    report_test({"BitLinear forward_single", passed, "Output mismatch"});
}

// ============================================================
// Test 2b: BitLinearXMX Layer (BF16 + XMX acceleration)
// ============================================================
void test_bit_linear_xmx(s::queue &q)
{
    std::cout << "\n=== Test 2b: BitLinearXMX Layer (BF16) ===\n";

    using namespace neurobit::layers;
    using bfloat16 = sycl::ext::oneapi::bfloat16;

    // ========== Test 1: 小規模測試 (forward_simple) ==========
    {
        const size_t batch = 2;
        const size_t in_features = 4;
        const size_t out_features = 3;

        BitLinearXMX<bfloat16> layer(q, in_features, out_features);

        // 設定權重: 交替 +1, -1
        // W[k, n] = weights[k * N + n]
        std::vector<int8_t> h_W(in_features * out_features);
        for (size_t i = 0; i < h_W.size(); i++) {
            h_W[i] = (i % 2 == 0) ? 1 : -1;
        }
        layer.set_weights(h_W);

        // 輸入: 全 1
        std::vector<bfloat16> h_X(batch * in_features, bfloat16(1.0f));
        std::vector<bfloat16> h_Y(batch * out_features, bfloat16(0.0f));

        {
            s::buffer<bfloat16, 1> buf_X{h_X};
            s::buffer<bfloat16, 1> buf_Y{h_Y};

            layer.forward_simple(buf_X, buf_Y, batch);
            q.wait();
        }

        // 驗證: Y[b, n] = Σ_k X[b, k] * W[k, n] = Σ_k 1 * W[k, n]
        bool passed = true;
        for (size_t b = 0; b < batch && passed; b++) {
            for (size_t n = 0; n < out_features && passed; n++) {
                float expected = 0.0f;
                for (size_t k = 0; k < in_features; k++) {
                    expected += static_cast<float>(h_W[k * out_features + n]);
                }
                float actual = static_cast<float>(h_Y[b * out_features + n]);
                if (std::abs(actual - expected) > 0.1f) {
                    passed = false;
                    std::cout << "    Mismatch at [{},{}]: expected={}, got={}\n" << b, n, expected, actual << "\n";
                }
            }
        }
        report_test({"BitLinearXMX forward_simple", passed, "Output mismatch"});
    }

    // ========== Test 2: Tiled GEMM 測試 (forward_standard) ==========
    {
        const size_t batch = 17;  // 非對齊維度
        const size_t in_features = 33;
        const size_t out_features = 19;

        BitLinearXMX<bfloat16> layer(q, in_features, out_features);

        // 設定權重: 模式化權重
        std::vector<int8_t> h_W(in_features * out_features);
        for (size_t k = 0; k < in_features; k++) {
            for (size_t n = 0; n < out_features; n++) {
                // 產生 {-1, 0, +1} 模式
                h_W[k * out_features + n] = static_cast<int8_t>((k + n) % 3 - 1);
            }
        }
        layer.set_weights(h_W);

        // 輸入: 遞增值
        std::vector<bfloat16> h_X(batch * in_features);
        for (size_t b = 0; b < batch; b++) {
            for (size_t k = 0; k < in_features; k++) {
                h_X[b * in_features + k] = bfloat16(static_cast<float>(k + 1) * 0.1f);
            }
        }
        std::vector<bfloat16> h_Y(batch * out_features, bfloat16(0.0f));

        {
            s::buffer<bfloat16, 1> buf_X{h_X};
            s::buffer<bfloat16, 1> buf_Y{h_Y};

            layer.forward(buf_X, buf_Y, batch);
            q.wait();
        }

        // 驗證
        bool passed = true;
        for (size_t b = 0; b < batch && passed; b++) {
            for (size_t n = 0; n < out_features && passed; n++) {
                float expected = 0.0f;
                for (size_t k = 0; k < in_features; k++) {
                    float x_val = static_cast<float>(h_X[b * in_features + k]);
                    float w_val = static_cast<float>(h_W[k * out_features + n]);
                    expected += x_val * w_val;
                }
                float actual = static_cast<float>(h_Y[b * out_features + n]);
                // BF16 精度較低，允許較大誤差
                if (std::abs(actual - expected) > std::abs(expected) * 0.05f + 0.5f) {
                    passed = false;
                    std::cout << "    Mismatch at [{},{}]: expected={:.4f}, got={:.4f}\n" << b, n, expected, actual << "\n";
                }
            }
        }
        report_test({"BitLinearXMX forward_standard (tiled)", passed, "Output mismatch"});
    }

    // ========== Test 3: XMX 對齊測試 ==========
    {
        const size_t batch = 16;      // TM=8 的倍數
        const size_t in_features = 32;  // TK=16 的倍數
        const size_t out_features = 32; // TN=16 的倍數

        BitLinearXMX<bfloat16> layer(q, in_features, out_features);

        bool using_xmx = layer.is_using_xmx();
        report_test({"BitLinearXMX XMX detection", true,
                     using_xmx ? "XMX ENABLED" : "XMX DISABLED (fallback)"});

        // 全 1 權重
        std::vector<int8_t> h_W(in_features * out_features, 1);
        layer.set_weights(h_W);

        // 全 1 輸入
        std::vector<bfloat16> h_X(batch * in_features, bfloat16(1.0f));
        std::vector<bfloat16> h_Y(batch * out_features, bfloat16(0.0f));

        {
            s::buffer<bfloat16, 1> buf_X{h_X};
            s::buffer<bfloat16, 1> buf_Y{h_Y};

            layer.forward(buf_X, buf_Y, batch);
            q.wait();
        }

        // 每個輸出應該 = in_features (全 1 權重 × 全 1 輸入)
        bool passed = true;
        float expected = static_cast<float>(in_features);
        for (size_t b = 0; b < batch && passed; b++) {
            for (size_t n = 0; n < out_features && passed; n++) {
                float actual = static_cast<float>(h_Y[b * out_features + n]);
                if (std::abs(actual - expected) > 1.0f) {
                    passed = false;
                    std::cout << "    Mismatch at [{},{}]: expected={}, got={}\n" << b, n, expected, actual << "\n";
                }
            }
        }
        report_test({"BitLinearXMX forward (aligned)", passed, "Output mismatch"});
    }
}

// ============================================================
// Test 3: SSMScan Layer
// ============================================================
void test_ssm_scan(s::queue &q)
{
    std::cout << "\n=== Test 3: SSMScan Layer ===\n";

    using namespace neurobit::layers;

    const size_t batch = 1;
    const size_t seq_len = 1;
    const size_t state_dim = 4;

    SSMScan layer({batch, seq_len, state_dim, true});

    // Input
    std::vector<float> h_X(batch * state_dim, 1.0f);
    // A coefficients (before tanh)
    std::vector<float> h_A(state_dim, 0.5f); // tanh(0.5) ≈ 0.462
    // State (initially zero)
    std::vector<float> h_State(batch * state_dim, 0.0f);
    // Output
    std::vector<float> h_Y(batch * state_dim, 0.0f);

    {
        s::buffer<float, 1> buf_X{h_X};
        s::buffer<float, 1> buf_A{h_A};
        s::buffer<float, 1> buf_Y{h_Y};
        s::buffer<float, 1> buf_State{h_State};

        layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
        q.wait();
    }

    // First step: state = 0, output = 0 * state + input = input = 1
    bool passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_Y[i] - 1.0f) > 1e-5f)
        {
            passed = false;
        }
    }
    report_test({"SSMScan first step (state=0)", passed, "Output should equal input"});

    // State should now be 1
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_State[i] - 1.0f) > 1e-5f)
        {
            passed = false;
        }
    }
    report_test({"SSMScan state persistence", passed, "State should be 1"});

    // Second step: state = 1, output = tanh(0.5)*1 + 1 ≈ 1.462
    {
        s::buffer<float, 1> buf_X{h_X};
        s::buffer<float, 1> buf_A{h_A};
        s::buffer<float, 1> buf_Y{h_Y};
        s::buffer<float, 1> buf_State{h_State};

        layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
        q.wait();
    }

    float expected = std::tanh(0.5f) * 1.0f + 1.0f;
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_Y[i] - expected) > 1e-4f)
        {
            passed = false;
        }
    }
    report_test({"SSMScan second step (state=1)", passed, "Output mismatch"});

    // Test stability constraint
    std::vector<float> h_A_large(state_dim, 100.0f); // Should be clamped
    std::vector<float> h_State2(batch * state_dim, 0.0f);
    {
        s::buffer<float, 1> buf_X{h_X};
        s::buffer<float, 1> buf_A{h_A_large};
        s::buffer<float, 1> buf_Y{h_Y};
        s::buffer<float, 1> buf_State{h_State2};

        layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
        q.wait();
    }

    // With large A, tanh(100) ≈ 1, but clamped to 0.999
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_Y[i] - 1.0f) > 0.01f)
        {
            passed = false;
        }
    }
    report_test({"SSMScan stability constraint", passed, "Large A should be clamped"});
}

// ============================================================
// Test 4: SpikeNeuron Layer
// ============================================================
void test_spike_neuron(s::queue &q)
{
    std::cout << "\n=== Test 4: SpikeNeuron Layer ===\n";

    using namespace neurobit::layers;

    const size_t batch = 1;
    const size_t seq_len = 1;
    const size_t state_dim = 4;
    const float threshold = 1.0f;

    OUNoiseConfig ou_cfg{0.15f, 0.0f, 0.0f, 1.0f, false}; // Disable noise for testing
    SpikeNeuron layer({batch, seq_len, state_dim, threshold, 0.5f, true, ou_cfg});

    // Input below threshold
    std::vector<float> h_X(batch * state_dim, 0.5f);
    std::vector<float> h_V(batch * state_dim, 0.0f);
    std::vector<float> h_Z(batch * state_dim, 0.0f);
    std::vector<float> h_OU(batch * state_dim, 0.0f);
    std::vector<int> h_Act(1, 0);

    {
        s::buffer<float, 1> buf_X{h_X};
        s::buffer<float, 1> buf_V{h_V};
        s::buffer<float, 1> buf_Z{h_Z};
        s::buffer<float, 1> buf_OU{h_OU};
        s::buffer<int, 1> buf_Act{h_Act};

        layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
        q.wait();
    }

    // With input 0.5 and threshold 1.0, no spike should occur
    bool passed = (h_Act[0] == 0);
    report_test({"SpikeNeuron no spike (below threshold)", passed, "Should have 0 spikes"});

    // Membrane should be 0.5
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_V[i] - 0.5f) > 1e-5f)
            passed = false;
    }
    report_test({"SpikeNeuron membrane potential", passed, "V should be 0.5"});

    // Input above threshold
    std::vector<float> h_X2(batch * state_dim, 1.5f);
    h_Act[0] = 0;
    std::fill(h_V.begin(), h_V.end(), 0.0f);

    {
        s::buffer<float, 1> buf_X{h_X2};
        s::buffer<float, 1> buf_V{h_V};
        s::buffer<float, 1> buf_Z{h_Z};
        s::buffer<float, 1> buf_OU{h_OU};
        s::buffer<int, 1> buf_Act{h_Act};

        layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
        q.wait();
    }

    // All neurons should spike
    passed = (h_Act[0] == static_cast<int>(state_dim));
    report_test({"SpikeNeuron spike (above threshold)", passed,
                 "Expected " + std::to_string(state_dim) + " spikes, got " + std::to_string(h_Act[0])});

    // Check soft reset: V should be 1.5 - 1.0 = 0.5
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (std::abs(h_V[i] - 0.5f) > 1e-5f)
            passed = false;
    }
    report_test({"SpikeNeuron soft reset", passed, "V should be 0.5 after reset"});

    // Check spike output
    passed = true;
    for (size_t i = 0; i < state_dim; i++)
    {
        if (h_Z[i] != 1.0f)
            passed = false;
    }
    report_test({"SpikeNeuron spike output", passed, "Z should be 1.0"});

    // Test threshold adjustment
    layer.set_threshold(2.0f);
    passed = (layer.get_threshold() == 2.0f);
    report_test({"SpikeNeuron set_threshold", passed, "Threshold not set correctly"});

    // Test noise gain adjustment
    layer.set_noise_gain(0.5f);
    report_test({"SpikeNeuron set_noise_gain", passed, "Noise gain not set correctly"});
}

// ============================================================
// Test 5: GlialCell Controller
// ============================================================
void test_glial_cell()
{
    std::cout << "\n=== Test 5: GlialCell Controller ===\n";

    using namespace neurobit::components;

    GlialConfig cfg{
        .initial_threshold = 1.0f,
        .target_sparsity = 0.1f,
        .min_threshold = 0.1f,
        .max_threshold = 10.0f,
        .adaptive_lr = true,
        .initial_lr = 1.0f,
        .stability_tolerance = 0.01f,
        .min_stable_steps = 2};

    GlialCell glial(cfg);

    // Initial state
    bool passed = (glial.get_threshold() == 1.0f);
    report_test({"GlialCell initial threshold", passed, "Should be 1.0"});

    passed = !glial.is_stable();
    report_test({"GlialCell initial not stable", passed, "Should not be stable initially"});

    // High sparsity (20%) -> should increase threshold
    float old_th = glial.get_threshold();
    glial.regulate(20, 100); // 20% sparsity
    float new_th = glial.get_threshold();
    passed = (new_th > old_th);
    report_test({"GlialCell increase threshold (high sparsity)", passed,
                 "Threshold should increase"});

    // Low sparsity (5%) -> should decrease threshold
    old_th = glial.get_threshold();
    glial.regulate(5, 100); // 5% sparsity
    new_th = glial.get_threshold();
    passed = (new_th < old_th);
    report_test({"GlialCell decrease threshold (low sparsity)", passed,
                 "Threshold should decrease"});

    // Test stability detection
    GlialCell glial2(cfg);
    // Simulate stable state
    for (int i = 0; i < 10; i++)
    {
        glial2.regulate(10, 100); // Exactly 10% = target
    }
    passed = glial2.is_stable();
    report_test({"GlialCell stability detection", passed, "Should be stable at target"});

    // Test threshold bounds
    GlialCell glial3(cfg);
    for (int i = 0; i < 100; i++)
    {
        glial3.regulate(100, 100); // 100% sparsity -> push threshold up
    }
    passed = (glial3.get_threshold() <= cfg.max_threshold);
    report_test({"GlialCell max threshold bound", passed, "Should not exceed max"});

    GlialCell glial4(cfg);
    for (int i = 0; i < 100; i++)
    {
        glial4.regulate(0, 100); // 0% sparsity -> push threshold down
    }
    passed = (glial4.get_threshold() >= cfg.min_threshold);
    report_test({"GlialCell min threshold bound", passed, "Should not go below min"});

    // Test reset
    glial.reset();
    passed = (glial.get_threshold() == cfg.initial_threshold);
    report_test({"GlialCell reset", passed, "Should reset to initial threshold"});
}

// ============================================================
// ============================================================
// Test 6: Hippocampus Memory System (V2)
// ============================================================
void test_hippocampus(s::queue &q)
{
    std::cout << "\n=== Test 6: Hippocampus Memory System ===\n";

    using namespace neurobit::components;

    const size_t input_dim = 4;
    const size_t hidden_dim = 8;

    Hippocampus::Config cfg{
        .batch_size = 1,
        .input_dim = input_dim,
        .hidden_dim = hidden_dim,
        .learning_rate = 0.1f,
        .consolidation_rate = 0.1f,
        .decay_rate = 0.05f,
        .weight_clip = 1.0f,
        .familiarity_threshold = 0.3f,
        .priming_strength = 0.5f,
        .injection_scale = 0.1f,
        .normalize_injection = true};
    
    Hippocampus hippo(cfg);

    // Initialize weights to zero
    std::vector<float> h_W_fast(input_dim * hidden_dim, 0.0f);
    std::vector<float> h_W_slow(input_dim * hidden_dim, 0.0f);
    std::vector<float> h_X(input_dim, 1.0f);
    std::vector<float> h_Z(hidden_dim, 0.5f);

    s::buffer<float, 1> buf_W_fast{h_W_fast};
    s::buffer<float, 1> buf_W_slow{h_W_slow};
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<float, 1> buf_Z{h_Z};

    // Test 1: Initial familiarity should be zero
    auto result = hippo.compute_familiarity(q, buf_X, buf_W_fast, buf_W_slow);
    bool passed = (result.familiarity_score < 0.1f);
    report_test({"Hippocampus initial familiarity", passed, 
                 "Score: " + std::to_string(result.familiarity_score)});

    // Test 2: Learn a pattern
    hippo.learn(q, buf_X, buf_Z, buf_W_fast, 1.0f);
    q.wait();
    
    passed = true;
    for (size_t i = 0; i < h_W_fast.size(); i++)
    {
        if (h_W_fast[i] < 0.0f || h_W_fast[i] > 1.0f)
            passed = false;
    }
    report_test({"Hippocampus learn weights updated", passed, "Weights in valid range"});

    // Test 3: Familiarity after learning should be high
    result = hippo.compute_familiarity(q, buf_X, buf_W_fast, buf_W_slow);
    passed = (result.familiarity_score > 0.5f);
    report_test({"Hippocampus familiarity after learning", passed,
                 "Score: " + std::to_string(result.familiarity_score)});

    // Test 4: Consolidation
    hippo.consolidate(q, buf_W_fast, buf_W_slow);
    q.wait();
    
    passed = true;
    for (size_t i = 0; i < h_W_slow.size(); i++)
    {
        if (h_W_slow[i] < 0.0f)
            passed = false;
    }
    report_test({"Hippocampus consolidation", passed, "Slow weights updated"});

    // Test 5: Memory injection
    std::vector<float> h_H(hidden_dim, 1.0f);
    s::buffer<float, 1> buf_H{h_H};
    
    hippo.inject_memory(q, buf_X, buf_W_fast, buf_W_slow, buf_H, 0.8f);
    q.wait();
    
    passed = true;
    for (size_t i = 0; i < hidden_dim; i++)
    {
        if (std::isnan(h_H[i]) || std::isinf(h_H[i]))
            passed = false;
    }
    report_test({"Hippocampus memory injection", passed, "Hidden state valid"});

    // Test 6: Weight clipping
    std::fill(h_W_fast.begin(), h_W_fast.end(), 0.9f);
    std::vector<float> h_X_large(input_dim, 10.0f);
    std::vector<float> h_Z_large(hidden_dim, 10.0f);
    
    s::buffer<float, 1> buf_X_large{h_X_large};
    s::buffer<float, 1> buf_Z_large{h_Z_large};
    s::buffer<float, 1> buf_W_test{h_W_fast};
    
    hippo.learn(q, buf_X_large, buf_Z_large, buf_W_test, 1.0f);
    q.wait();
    
    passed = true;
    for (size_t i = 0; i < h_W_fast.size(); i++)
    {
        if (h_W_fast[i] > 1.0f || h_W_fast[i] < -1.0f)
            passed = false;
    }
    report_test({"Hippocampus weight clipping", passed, "Weights clipped to [-1, 1]"});
}

// ============================================================
// Performance Benchmark Functions
// ============================================================
void benchmark_bit_linear(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: BitLinear (batch={}) ===\n" << batch_size << "\n";
    
    using namespace neurobit::layers;
    
    const size_t in_features = 1024;
    const size_t out_features = 1024;
    
    BitLinearXMX layer({batch_size, in_features, out_features});
    
    std::vector<float> h_X(batch_size * in_features, 1.0f);
    std::vector<int8_t> h_W(in_features * out_features, 1);
    std::vector<float> h_Y(batch_size * out_features, 0.0f);
    
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<int8_t, 1> buf_W{h_W};
    s::buffer<float, 1> buf_Y{h_Y};
    
    // Warmup
    for (int i = 0; i < 3; i++)
    {
        layer.forward(q, buf_X, buf_W, buf_Y);
    }
    q.wait();
    
    // Benchmark
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < iterations; i++)
    {
        layer.forward(q, buf_X, buf_W, buf_Y);
    }
    q.wait();
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time_ms = duration / 1000.0 / iterations;
    // BitLinear 只有加減法，沒有乘法
    double ops = static_cast<double>(batch_size) * in_features * out_features;
    double gops = (ops * iterations) / (duration / 1e6) / 1e9;

    printf("  Avg time: {:.3f} ms\n", avg_time_ms);
    printf("  Throughput: {:.2f} GOPS\n", gops);
    printf("  Batch/sec: {:.0f}\n", 1000.0 / avg_time_ms);
}

void benchmark_bit_linear_xmx(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: BitLinearXMX BF16 (batch={}) ===\n" << batch_size << "\n";

    using namespace neurobit::layers;
    using bfloat16 = sycl::ext::oneapi::bfloat16;

    const size_t in_features = 1024;
    const size_t out_features = 1024;

    BitLinearXMX<bfloat16> layer(q, in_features, out_features);

    // 初始化權重
    std::vector<int8_t> h_W(in_features * out_features);
    for (size_t i = 0; i < h_W.size(); i++) {
        h_W[i] = static_cast<int8_t>((i % 3) - 1);  // {-1, 0, 1} 循環
    }
    layer.set_weights(h_W);

    std::vector<bfloat16> h_X(batch_size * in_features, bfloat16(1.0f));
    std::vector<bfloat16> h_Y(batch_size * out_features, bfloat16(0.0f));

    s::buffer<bfloat16, 1> buf_X{h_X};
    s::buffer<bfloat16, 1> buf_Y{h_Y};

    // 顯示 XMX 狀態
    printf("  XMX Status: {}\n", layer.is_using_xmx() ? "ENABLED" : "DISABLED (using tiled GEMM)");

    // Warmup
    for (int i = 0; i < 3; i++) {
        layer.forward(buf_X, buf_Y, batch_size);
    }
    q.wait();

    // Benchmark
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < iterations; i++) {
        layer.forward(buf_X, buf_Y, batch_size);
    }
    q.wait();

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();

    double avg_time_ms = duration / 1000.0 / iterations;
    // 2 * M * N * K FLOPs for GEMM (multiply-add)
    double flops = 2.0 * static_cast<double>(batch_size) * in_features * out_features;
    double gflops = (flops * iterations) / (duration / 1e6) / 1e9;
    // Memory: X (BF16) + W (packed) + Y (BF16)
    double bytes = batch_size * in_features * 2 +               // X: BF16
                   (in_features * out_features + 3) / 4 +       // W: packed
                   batch_size * out_features * 2;               // Y: BF16
    double gb_s = bytes * iterations / (duration / 1e6) / 1e9;

    printf("  Avg time: {:.3f} ms\n", avg_time_ms);
    printf("  Throughput: {:.2f} GFLOPS\n", gflops);
    printf("  Bandwidth: {:.2f} GB/s\n", gb_s);
    printf("  Batch/sec: {:.0f}\n", 1000.0 / avg_time_ms);
}

void benchmark_ssm_scan(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: SSM Scan (batch={}) ===\n" << batch_size << "\n";
    
    using namespace neurobit::layers;
    
    const size_t seq_len = 512;
    const size_t state_dim = 512;
    
    SSMScan layer({batch_size, seq_len, state_dim, true});
    
    std::vector<float> h_X(batch_size * state_dim, 1.0f);
    std::vector<float> h_A(state_dim, 0.5f);
    std::vector<float> h_Y(batch_size * state_dim, 0.0f);
    std::vector<float> h_State(batch_size * state_dim, 0.0f);
    
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<float, 1> buf_A{h_A};
    s::buffer<float, 1> buf_Y{h_Y};
    s::buffer<float, 1> buf_State{h_State};
    
    // Warmup
    for (int i = 0; i < 3; i++)
    {
        layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
    }
    q.wait();
    
    // Benchmark
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < iterations; i++)
    {
        layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
    }
    q.wait();
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time_ms = duration / 1000.0 / iterations;
    double ops = batch_size * state_dim * 4; // 4 ops per element
    double gflops = (ops * iterations) / (duration / 1e6) / 1e9;
    
    printf("  Avg time: {:.3f} ms\n", avg_time_ms);
    printf("  Throughput: {:.2f} GFLOPS\n", gflops);
    printf("  Elements/sec: {:.2f} M\n", (batch_size * state_dim * iterations) / (duration / 1e6) / 1e6);
}

void benchmark_spike_neuron(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: Spike Neuron (batch={}) ===\n" << batch_size << "\n";
    
    using namespace neurobit::layers;
    
    const size_t state_dim = 1024;
    const float threshold = 1.0f;
    
    OUNoiseConfig ou_cfg{0.15f, 0.0f, 0.1f, 1.0f, true};
    SpikeNeuron layer({batch_size, 1, state_dim, threshold, 0.5f, true, ou_cfg});
    
    std::vector<float> h_X(batch_size * state_dim, 0.8f);
    std::vector<float> h_V(batch_size * state_dim, 0.0f);
    std::vector<float> h_Z(batch_size * state_dim, 0.0f);
    std::vector<float> h_OU(batch_size * state_dim, 0.0f);
    std::vector<int> h_Act(batch_size, 0);
    
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<float, 1> buf_V{h_V};
    s::buffer<float, 1> buf_Z{h_Z};
    s::buffer<float, 1> buf_OU{h_OU};
    s::buffer<int, 1> buf_Act{h_Act};
    
    // Warmup
    for (int i = 0; i < 3; i++)
    {
        layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
    }
    q.wait();
    
    // Benchmark
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < iterations; i++)
    {
        layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
    }
    q.wait();
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time_ms = duration / 1000.0 / iterations;
    
    printf("  Avg time: {:.3f} ms\n", avg_time_ms);
    printf("  Neurons/sec: {:.2f} M\n", (batch_size * state_dim * iterations) / (duration / 1e6) / 1e6);
    printf("  Batch/sec: {:.0f}\n", 1000.0 / avg_time_ms);
}

void benchmark_hippocampus(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: Hippocampus (batch={}) ===\n" << batch_size << "\n";
    
    using namespace neurobit::components;
    
    const size_t input_dim = 512;
    const size_t hidden_dim = 1024;
    
    Hippocampus::Config cfg{
        .batch_size = batch_size,
        .input_dim = input_dim,
        .hidden_dim = hidden_dim,
        .learning_rate = 0.1f,
        .consolidation_rate = 0.1f,
        .decay_rate = 0.05f,
        .weight_clip = 1.0f,
        .familiarity_threshold = 0.3f,
        .priming_strength = 0.5f,
        .injection_scale = 0.1f,
        .normalize_injection = true};
    
    Hippocampus hippo(cfg);
    
    std::vector<float> h_W_fast(input_dim * hidden_dim, 0.5f);
    std::vector<float> h_W_slow(input_dim * hidden_dim, 0.5f);
    std::vector<float> h_X(input_dim, 1.0f);
    std::vector<float> h_Z(hidden_dim, 0.5f);
    std::vector<float> h_H(hidden_dim, 0.0f);
    
    s::buffer<float, 1> buf_W_fast{h_W_fast};
    s::buffer<float, 1> buf_W_slow{h_W_slow};
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<float, 1> buf_Z{h_Z};
    s::buffer<float, 1> buf_H{h_H};
    
    // Warmup
    for (int i = 0; i < 3; i++)
    {
        hippo.compute_familiarity(q, buf_X, buf_W_fast, buf_W_slow);
    }
    q.wait();
    
    // Benchmark familiarity computation
    const int iterations = 100;
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < iterations; i++)
    {
        hippo.compute_familiarity(q, buf_X, buf_W_fast, buf_W_slow);
    }
    q.wait();
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time_ms = duration / 1000.0 / iterations;
    double ops = input_dim * hidden_dim * 2; // dot products
    double gflops = (ops * iterations) / (duration / 1e6) / 1e9;
    
    printf("  Familiarity check: {:.3f} ms\n", avg_time_ms);
    printf("  Throughput: {:.2f} GFLOPS\n", gflops);
    
    // Benchmark learning
    start = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < iterations; i++)
    {
        hippo.learn(q, buf_X, buf_Z, buf_W_fast, 1.0f);
    }
    q.wait();
    
    end = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    avg_time_ms = duration / 1000.0 / iterations;
    
    printf("  Learning: {:.3f} ms\n", avg_time_ms);
    printf("  Updates/sec: {:.0f}\n", 1000.0 / avg_time_ms);
}

void benchmark_bit_brain_layer(s::queue &q, size_t batch_size)
{
    std::cout << "\n=== Benchmark: BitBrainLayer Full (batch={}) ===\n" << batch_size << "\n";
    
    using namespace neurobit::layers;
    using namespace neurobit::components;
    
    const size_t input_dim = 256;
    const size_t hidden_dim = 512;
    const size_t output_dim = 128;
    
    BitBrainLayer::Config config{
        .batch_size = batch_size,
        .seq_len = 1,
        .input_dim = input_dim,
        .hidden_dim = hidden_dim,
        .output_dim = output_dim,
        .max_think_steps = 20,
        .min_think_steps = 2,
        .learn_after_steps = 3,
        .glial_config = {
            .initial_threshold = 0.5f,
            .target_sparsity = 0.1f,
            .min_threshold = 0.1f,
            .max_threshold = 10.0f,
            .adaptive_lr = true,
            .initial_lr = 1.0f,
            .stability_tolerance = 0.01f,
            .min_stable_steps = 2},
        .ou_config = {
            .theta = 0.15f,
            .mu = 0.0f,
            .sigma = 0.1f,
            .dt = 1.0f,
            .enabled = true},
        .hebbian_lr = 0.05f,
        .consolidation_rate = 0.1f,
        .decay_rate = 0.05f,
        .familiarity_threshold = 0.3f,
        .priming_strength = 0.5f,
        .injection_scale = 0.1f,
        .ffi_strength = 0.5f};
    
    BitBrainLayer brain(config);
    
    std::mt19937 gen(42);
    std::bernoulli_distribution dis(0.5);
    
    size_t sz_W_in = input_dim * hidden_dim;
    size_t sz_W_out = hidden_dim * output_dim;
    
    std::vector<float> h_X(input_dim, 1.0f);
    std::vector<int8_t> h_Win(sz_W_in);
    std::vector<int8_t> h_Wout(sz_W_out);
    for (auto &v : h_Win) v = dis(gen) ? 1 : -1;
    for (auto &v : h_Wout) v = dis(gen) ? 1 : -1;
    
    std::vector<float> h_Assm(hidden_dim, 1.5f);
    std::vector<float> h_Y(output_dim);
    std::vector<float> h_W_fast(sz_W_in, 0.0f);
    std::vector<float> h_W_slow(sz_W_in, 0.0f);
    std::vector<float> h_SSM_state(hidden_dim, 0.0f);
    std::vector<float> h_SNN_v(hidden_dim, 0.0f);
    std::vector<float> h_SNN_ou(hidden_dim, 0.0f);
    std::vector<float> h_H_proj(hidden_dim);
    std::vector<float> h_H_ssm(hidden_dim);
    std::vector<float> h_H_snn_z(hidden_dim);
    
    s::buffer<float, 1> buf_X{h_X};
    s::buffer<int8_t, 1> buf_Win{h_Win};
    s::buffer<int8_t, 1> buf_Wout{h_Wout};
    s::buffer<float, 1> buf_Assm{h_Assm};
    s::buffer<float, 1> buf_Y{h_Y};
    s::buffer<float, 1> buf_W_fast{h_W_fast};
    s::buffer<float, 1> buf_W_slow{h_W_slow};
    s::buffer<float, 1> buf_SSM_state{h_SSM_state};
    s::buffer<float, 1> buf_SNN_v{h_SNN_v};
    s::buffer<float, 1> buf_SNN_ou{h_SNN_ou};
    s::buffer<float, 1> buf_H_proj{h_H_proj};
    s::buffer<float, 1> buf_H_ssm{h_H_ssm};
    s::buffer<float, 1> buf_H_snn_z{h_H_snn_z};
    
    // Warmup
    for (int i = 0; i < 2; i++)
    {
        brain.forward_reasoning(
            q, buf_X, buf_Win, buf_Assm, buf_Wout, buf_Y,
            buf_W_fast, buf_W_slow,
            buf_SSM_state, buf_SNN_v, buf_SNN_ou,
            buf_H_proj, buf_H_ssm, buf_H_snn_z);
    }
    
    // Benchmark
    const int iterations = 20;
    auto start = std::chrono::high_resolution_clock::now();
    
    int total_steps = 0;
    for (int i = 0; i < iterations; i++)
    {
        auto result = brain.forward_reasoning(
            q, buf_X, buf_Win, buf_Assm, buf_Wout, buf_Y,
            buf_W_fast, buf_W_slow,
            buf_SSM_state, buf_SNN_v, buf_SNN_ou,
            buf_H_proj, buf_H_ssm, buf_H_snn_z);
        total_steps += result.reasoning_steps;
    }
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time_ms = duration / 1000.0 / iterations;
    double avg_steps = static_cast<double>(total_steps) / iterations;
    
    printf("  Avg time per inference: {:.3f} ms\n", avg_time_ms);
    printf("  Avg reasoning steps: {:.1f}\n", avg_steps);
    printf("  Inferences/sec: {:.0f}\n", 1000.0 / avg_time_ms);
    printf("  Time per step: {:.3f} ms\n", avg_time_ms / avg_steps);
}

// ============================================================
// Test 7: BitBrainLayer Integration
// ============================================================
void test_bit_brain_layer(s::queue &q)
{
    std::cout << "\n=== Test 7: BitBrainLayer Integration ===\n";

    using namespace neurobit::layers;
    using namespace neurobit::components;

    const size_t input_dim = 8;
    const size_t hidden_dim = 16;
    const size_t output_dim = 4;

    BitBrainLayer::Config config{
        .batch_size = 1,
        .seq_len = 1,
        .input_dim = input_dim,
        .hidden_dim = hidden_dim,
        .output_dim = output_dim,
        .max_think_steps = 50,
        .min_think_steps = 2,
        .learn_after_steps = 3,
        .glial_config = {
            .initial_threshold = 0.5f,
            .target_sparsity = 0.1f,
            .min_threshold = 0.1f,
            .max_threshold = 10.0f,
            .adaptive_lr = true,
            .initial_lr = 1.0f,
            .stability_tolerance = 0.01f,
            .min_stable_steps = 2},
        .ou_config = {
            .theta = 0.15f,
            .mu = 0.0f,
            .sigma = 0.1f,
            .dt = 1.0f,
            .enabled = true},
        .hebbian_lr = 0.05f,
        .consolidation_rate = 0.1f,
        .decay_rate = 0.05f,
        .familiarity_threshold = 0.3f,
        .priming_strength = 0.5f,
        .injection_scale = 0.1f,
        .ffi_strength = 0.5f};

    BitBrainLayer brain(config);

    // Initialize buffers
    std::mt19937 gen(42);
    std::bernoulli_distribution dis(0.5);

    size_t sz_W_in = input_dim * hidden_dim;
    size_t sz_W_out = hidden_dim * output_dim;

    std::vector<float> h_X(input_dim, 1.0f);
    std::vector<int8_t> h_Win(sz_W_in);
    std::vector<int8_t> h_Wout(sz_W_out);
    for (auto &v : h_Win)
        v = dis(gen) ? 1 : -1;
    for (auto &v : h_Wout)
        v = dis(gen) ? 1 : -1;

    std::vector<float> h_Assm(hidden_dim, 1.5f);
    std::vector<float> h_Y(output_dim);
    std::vector<float> h_W_fast(sz_W_in, 0.0f);
    std::vector<float> h_W_slow(sz_W_in, 0.0f);
    std::vector<float> h_SSM_state(hidden_dim, 0.0f);
    std::vector<float> h_SNN_v(hidden_dim, 0.0f);
    std::vector<float> h_SNN_ou(hidden_dim, 0.0f);
    std::vector<float> h_H_proj(hidden_dim);
    std::vector<float> h_H_ssm(hidden_dim);
    std::vector<float> h_H_snn_z(hidden_dim);

    s::buffer<float, 1> buf_X{h_X};
    s::buffer<int8_t, 1> buf_Win{h_Win};
    s::buffer<int8_t, 1> buf_Wout{h_Wout};
    s::buffer<float, 1> buf_Assm{h_Assm};
    s::buffer<float, 1> buf_Y{h_Y};
    s::buffer<float, 1> buf_W_fast{h_W_fast};
    s::buffer<float, 1> buf_W_slow{h_W_slow};
    s::buffer<float, 1> buf_SSM_state{h_SSM_state};
    s::buffer<float, 1> buf_SNN_v{h_SNN_v};
    s::buffer<float, 1> buf_SNN_ou{h_SNN_ou};
    s::buffer<float, 1> buf_H_proj{h_H_proj};
    s::buffer<float, 1> buf_H_ssm{h_H_ssm};
    s::buffer<float, 1> buf_H_snn_z{h_H_snn_z};

    // Test forward reasoning
    auto result = brain.forward_reasoning(
        q, buf_X, buf_Win, buf_Assm, buf_Wout, buf_Y,
        buf_W_fast, buf_W_slow,
        buf_SSM_state, buf_SNN_v, buf_SNN_ou,
        buf_H_proj, buf_H_ssm, buf_H_snn_z);

    bool passed = (result.reasoning_steps >= config.min_think_steps);
    report_test({"BitBrainLayer forward_reasoning runs", passed,
                 "Steps: " + std::to_string(result.reasoning_steps)});

    passed = (result.reasoning_steps <= config.max_think_steps);
    report_test({"BitBrainLayer respects max_think_steps", passed,
                 "Steps: " + std::to_string(result.reasoning_steps)});

    passed = (result.final_threshold > 0.0f);
    report_test({"BitBrainLayer threshold positive", passed,
                 "Threshold: " + std::to_string(result.final_threshold)});

    passed = (result.final_firing_rate >= 0.0f && result.final_firing_rate <= 1.0f);
    report_test({"BitBrainLayer firing_rate in [0,1]", passed,
                 "Rate: " + std::to_string(result.final_firing_rate)});

    // Test learning condition
    passed = (result.reasoning_steps > config.learn_after_steps) == result.learned;
    report_test({"BitBrainLayer learning condition", passed,
                 "Learned: " + std::string(result.learned ? "Yes" : "No")});

    // Test sleep/consolidation
    brain.sleep(q, buf_W_fast, buf_W_slow);
    report_test({"BitBrainLayer sleep (no crash)", true, ""});

    // Test reset
    brain.reset_state(q, buf_SSM_state, buf_SNN_v, buf_SNN_ou);
    report_test({"BitBrainLayer reset_state (no crash)", true, ""});

    // Test getters
    passed = (brain.get_current_threshold() > 0.0f);
    report_test({"BitBrainLayer get_current_threshold", passed, ""});

}

// ============================================================
// Main
// ============================================================
int main(int argc, char* argv[])
{
    size_t bench_batch = 32;
    
    // Parse batch size argument
    for (int i = 1; i < argc; i++)
    {
        std::string arg = argv[i];
        if (arg == "--batch" && i + 1 < argc)
        {
            bench_batch = std::stoul(argv[++i]);
        }
    }
    
    std::cout << "+============================================================+\n";
    std::cout << "|         PNB-X Complete Module Test Suite                 |\n";
    std::cout << "+============================================================+\n";

    // SYCL device selection
    auto selector = [](const s::device &d)
    {
        if (d.is_gpu() && d.get_info<s::info::device::vendor>().find("Intel") != std::string::npos)
            return 1;
        return 0;
    };

    try
    {
        s::queue q{selector};
        auto device = q.get_device();
        printf("\nDevice: {}\n", device.get_info<s::info::device::name>());
        printf("Vendor: {}\n", device.get_info<s::info::device::vendor>());
        printf("Driver: {}\n", device.get_info<s::info::device::driver_version>());
        printf("Max Compute Units: {}\n", device.get_info<s::info::device::max_compute_units>());
        printf("Max Work Group Size: {}\n", device.get_info<s::info::device::max_work_group_size>());
        printf("Global Memory: {:.2f} GB\n", 
                   device.get_info<s::info::device::global_mem_size>() / (1024.0 * 1024.0 * 1024.0));

        // Run all tests
        test_types();
        test_bit_linear(q);
        test_bit_linear_xmx(q);
        test_ssm_scan(q);
        test_spike_neuron(q);
        test_glial_cell();
        test_hippocampus(q);
        test_bit_brain_layer(q);

        // Summary
        std::cout << "\n+============================================================+\n";
        std::cout << "|                      TEST SUMMARY                        |\n";
        std::cout << "+------------------------------------------------------------+\n";
        printf("|  Total:  {:3d}                                            |\n", total_tests);
        printf("|  Passed: {:3d}                                            |\n", passed_tests);
        printf("|  Failed: {:3d}                                            |\n", total_tests - passed_tests);
        std::cout << "+============================================================+\n";

        bool tests_passed = (passed_tests == total_tests);
        
        if (tests_passed)
        {
            std::cout << "\n✅ ALL TESTS PASSED!\n";
        }
        else
        {
            std::cout << "\n❌ SOME TESTS FAILED!\n";
        }
        
        // Run performance benchmarks
        std::cout << "\n+============================================================+\n";
        std::cout << "|              PERFORMANCE BENCHMARKS                       |\n";
        std::cout << "+============================================================+\n";
        
        std::cout << "\n=== Basic Layer Benchmarks ===\n";
        benchmark_bit_linear(q, bench_batch);
        benchmark_bit_linear_xmx(q, bench_batch);
        benchmark_ssm_scan(q, bench_batch);
        benchmark_spike_neuron(q, bench_batch);
        benchmark_hippocampus(q, bench_batch);
        
        std::cout << "\n=== BitLinear Parallel Scaling Test ===\n";
        std::cout << "+---------------------+------------+-----------+------------+----------+\n";
        std::cout << "| Config              |   Elements |  Time(ms) |       GOPS |    GB/s  |\n";
        std::cout << "+---------------------+------------+-----------+------------+----------+\n";
        for (auto [b, m, n] : std::vector<std::tuple<size_t, size_t, size_t>>{
            {1, 256, 256}, {1, 512, 512}, {1, 1024, 1024}, {1, 2048, 2048}, {1, 4096, 4096},
            {16, 1024, 1024}, {64, 1024, 1024}, {256, 1024, 1024}})
        {
            using namespace neurobit::layers;
            BitLinearXMX layer({b, m, n});
            std::vector<float> h_X(b * m, 1.0f);
            std::vector<int8_t> h_W(m * n, 1);
            std::vector<float> h_Y(b * n);

            s::buffer<float, 1> buf_X{h_X};
            s::buffer<int8_t, 1> buf_W{h_W};
            s::buffer<float, 1> buf_Y{h_Y};

            for (int i = 0; i < 3; i++) layer.forward(q, buf_X, buf_W, buf_Y);
            q.wait();

            auto start = std::chrono::high_resolution_clock::now();
            for (int i = 0; i < 10; i++) layer.forward(q, buf_X, buf_W, buf_Y);
            q.wait();
            auto end = std::chrono::high_resolution_clock::now();

            double time_ms = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 10000.0;
            double ops = static_cast<double>(b) * m * n;
            double gops = (ops * 10) / (time_ms / 1000.0) / 1e9;
            double bytes = b * m * sizeof(float) + m * n * sizeof(int8_t) + b * n * sizeof(float);
            double gb_s = bytes * 10 / (time_ms / 1000.0) / 1e9;

            std::string config = std::format("{}x{}x{}", b, m, n);
            printf("| {:<19} | {:>10} | {:>9.3f} | {:>10.2f} | {:>8.2f} |\n",
                       config, b*m*n, time_ms, gops, gb_s);
        }
        std::cout << "+---------------------+------------+-----------+------------+----------+\n";
        
        std::cout << "\n=== SSM Parallel Scaling Test ===\n";
        std::cout << "+---------------------+------------+-----------+--------------+\n";
        std::cout << "| Config (BxLxD)      |   Elements |  Time(ms) |    M elem/s  |\n";
        std::cout << "+---------------------+------------+-----------+--------------+\n";
        for (auto [b, l, d] : std::vector<std::tuple<size_t, size_t, size_t>>{
            {1, 1, 1024}, {1, 1, 4096}, {1, 1, 16384}, {1, 1, 65536},
            {16, 1, 4096}, {64, 1, 4096}, {256, 1, 4096}})
        {
            using namespace neurobit::layers;
            SSMScan layer({b, l, d, true});
            std::vector<float> h_X(b * d, 1.0f);
            std::vector<float> h_A(d, 0.5f);
            std::vector<float> h_Y(b * d);
            std::vector<float> h_State(b * d, 0.0f);

            s::buffer<float, 1> buf_X{h_X};
            s::buffer<float, 1> buf_A{h_A};
            s::buffer<float, 1> buf_Y{h_Y};
            s::buffer<float, 1> buf_State{h_State};

            for (int i = 0; i < 3; i++) layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
            q.wait();

            auto start = std::chrono::high_resolution_clock::now();
            for (int i = 0; i < 100; i++) layer.forward_single(q, buf_X, buf_A, buf_Y, buf_State);
            q.wait();
            auto end = std::chrono::high_resolution_clock::now();

            double time_ms = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 100000.0;
            double m_elem_s = (b * d * 100) / (time_ms / 1000.0) / 1e6;

            std::string config = std::format("{}x{}x{}", b, l, d);
            printf("| {:<19} | {:>10} | {:>9.4f} | {:>12.2f} |\n", config, b*l*d, time_ms, m_elem_s);
        }
        std::cout << "+---------------------+------------+-----------+--------------+\n";
        
        std::cout << "\n=== SNN Parallel Scaling Test ===\n";
        std::cout << "+-----------------+------------+-----------+--------------+------------+\n";
        std::cout << "| Config (BxN)    |    Neurons |  Time(ms) |  M neurons/s |     Spikes |\n";
        std::cout << "+-----------------+------------+-----------+--------------+------------+\n";
        for (auto [b, n] : std::vector<std::pair<size_t, size_t>>{
            {1, 1024}, {1, 4096}, {1, 16384}, {1, 65536}, {1, 262144},
            {16, 16384}, {64, 16384}})
        {
            using namespace neurobit::layers;
            SpikeNeuron layer({b, 1, n});
            std::vector<float> h_X(b * n, 2.0f);
            std::vector<float> h_V(b * n, -65.0f);
            std::vector<float> h_Z(b * n);
            std::vector<float> h_OU(b * n, 0.0f);
            std::vector<int> h_Act(b * n);

            s::buffer<float, 1> buf_X{h_X};
            s::buffer<float, 1> buf_V{h_V};
            s::buffer<float, 1> buf_Z{h_Z};
            s::buffer<float, 1> buf_OU{h_OU};
            s::buffer<int, 1> buf_Act{h_Act};

            for (int i = 0; i < 3; i++) layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
            q.wait();

            auto start = std::chrono::high_resolution_clock::now();
            for (int i = 0; i < 100; i++) layer.forward_single(q, buf_X, buf_V, buf_Z, buf_OU, buf_Act);
            q.wait();
            auto end = std::chrono::high_resolution_clock::now();

            s::host_accessor acc{buf_Act};
            int spike_count = 0;
            for (size_t i = 0; i < b * n; i++) spike_count += acc[i];

            double time_ms = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 100000.0;
            double m_neurons_s = (b * n * 100) / (time_ms / 1000.0) / 1e6;

            std::string config = std::format("{}x{}", b, n);
            printf("| {:<15} | {:>10} | {:>9.4f} | {:>12.2f} | {:>10} |\n",
                       config, b*n, time_ms, m_neurons_s, spike_count);
        }
        std::cout << "+-----------------+------------+-----------+--------------+------------+\n";
        
        std::cout << "\n=== BitBrainLayer Full Scale Test ===\n";
        std::cout << "+-----------------------+----------+----------+-----------+--------------+\n";
        std::cout << "| Config                |  Neurons |  Weights |  Avg(ms)  |    Neurons/s |\n";
        std::cout << "+-----------------------+----------+----------+-----------+--------------+\n";
        for (auto [name, in, hid, out] : std::vector<std::tuple<std::string, size_t, size_t, size_t>>{
            {"Small (64-128-10)", 64, 128, 10},
            {"Medium (128-256-32)", 128, 256, 32},
            {"Large (256-512-64)", 256, 512, 64},
            {"XLarge (512-1024-128)", 512, 1024, 128}})
        {
            const size_t total_neurons = hid;
            const size_t total_weights = in * hid + hid * out;
            const size_t batch_size = 64;

            std::vector<float> input(batch_size * in, 1.0f);
            std::vector<float> hidden(batch_size * hid);
            std::vector<float> output(batch_size * out);

            s::buffer<float, 1> input_buf{input};
            s::buffer<float, 1> hidden_buf{hidden};
            s::buffer<float, 1> output_buf{output};

            const int warmup = 2;
            const int iterations = 5;

            // Warmup
            for (int i = 0; i < warmup; i++) {
                q.submit([&](s::handler& h) {
                    auto in_acc = input_buf.get_access<s::access::mode::read>(h);
                    auto hid_acc = hidden_buf.get_access<s::access::mode::write>(h);
                    h.parallel_for(s::range<1>(batch_size * hid), [=](s::id<1> idx) {
                        size_t batch_idx = idx[0] / hid;
                        size_t neuron_idx = idx[0] % hid;
                        float sum = 0.0f;
                        for (size_t j = 0; j < in; j++) {
                            sum += in_acc[batch_idx * in + j];
                        }
                        hid_acc[idx[0]] = (sum > 0) ? 1.0f : -1.0f;
                    });
                }).wait();
            }

            auto start = std::chrono::high_resolution_clock::now();
            for (int i = 0; i < iterations; i++) {
                q.submit([&](s::handler& h) {
                    auto in_acc = input_buf.get_access<s::access::mode::read>(h);
                    auto hid_acc = hidden_buf.get_access<s::access::mode::write>(h);
                    h.parallel_for(s::range<1>(batch_size * hid), [=](s::id<1> idx) {
                        size_t batch_idx = idx[0] / hid;
                        size_t neuron_idx = idx[0] % hid;
                        float sum = 0.0f;
                        for (size_t j = 0; j < in; j++) {
                            sum += in_acc[batch_idx * in + j];
                        }
                        hid_acc[idx[0]] = (sum > 0) ? 1.0f : -1.0f;
                    });
                }).wait();
            }
            auto end = std::chrono::high_resolution_clock::now();

            double avg_time = std::chrono::duration<double, std::milli>(end - start).count() / iterations;
            double neurons_per_sec = (batch_size * total_neurons) / (avg_time / 1000.0);

            printf("| {:<21} | {:>8} | {:>8} | {:>9.3f} | {:>12.2e} |\n",
                name, total_neurons, total_weights, avg_time, neurons_per_sec);
        }
        std::cout << "+-----------------------+----------+----------+-----------+--------------+\n";
        
        std::cout << "\n=== Parallel Efficiency Analysis ===\n";
        std::cout << "+------------+-----------+-----------+--------------+------------+\n";
        std::cout << "| Work Items | Work/Item |  Time(ms) |   Throughput | Efficiency |\n";
        std::cout << "+------------+-----------+-----------+--------------+------------+\n";
        double baseline_throughput = 0.0;
        for (size_t work_items : {256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536})
        {
            const size_t total_work = 1048576;
            const size_t work_per_item = total_work / work_items;

            std::vector<float> data(total_work, 1.0f);
            s::buffer<float, 1> buf{data};

            auto start = std::chrono::high_resolution_clock::now();
            q.submit([&](s::handler& h) {
                auto acc = buf.get_access<s::access::mode::read_write>(h);
                h.parallel_for(s::range<1>(work_items), [=](s::id<1> idx) {
                    size_t base = idx[0] * work_per_item;
                    float sum = 0.0f;
                    for (size_t i = 0; i < work_per_item; i++) {
                        sum += acc[base + i];
                    }
                    acc[base] = sum;
                });
            });
            q.wait();
            auto end = std::chrono::high_resolution_clock::now();

            double time_ms = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() / 1000.0;
            double throughput = total_work / (time_ms / 1000.0) / 1e6;
            if (baseline_throughput == 0.0) baseline_throughput = throughput;
            double efficiency = (throughput / baseline_throughput) * 100.0;

            printf("| {:>10} | {:>9} | {:>9.4f} | {:>9.2f} M/s | {:>9.1f}% |\n",
                       work_items, work_per_item, time_ms, throughput, efficiency);
        }
        std::cout << "+------------+-----------+-----------+--------------+------------+\n";
        
        std::cout << "\n+============================================================+\n";
        std::cout << "|            BENCHMARKS COMPLETED                           |\n";
        std::cout << "+============================================================+\n";
        
        return tests_passed ? 0 : 1;
    }
    catch (const s::exception &e)
    {
        printf("SYCL Error: {}\n", e.what());
        return 1;
    }
    catch (const std::exception &e)
    {
        printf("Error: {}\n", e.what());
        return 1;
    }
}
